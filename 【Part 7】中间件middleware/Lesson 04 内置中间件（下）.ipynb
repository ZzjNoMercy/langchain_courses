{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6afc6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8560f",
   "metadata": {},
   "source": [
    "# LLMToolSelectorMiddleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11da44c",
   "metadata": {},
   "source": [
    "ä¸€ç§å¸¸è§çš„å‹ç¼©æ‰‹æ®µï¼Œåœ¨è°ƒç”¨ä¸»æ¨¡å‹ä¹‹å‰ï¼Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½åœ°é€‰æ‹©ç›¸å…³å·¥å…·ã€‚LLM å·¥å…·é€‰æ‹©å™¨é€‚ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š\n",
    "- æ‹¥æœ‰å¤§é‡å·¥å…·ï¼ˆ10 ä¸ªä»¥ä¸Šï¼‰çš„æ™ºèƒ½ä½“ï¼Œå…¶ä¸­å¤§å¤šæ•°å·¥å…·å¯¹æ¯ä¸ªæŸ¥è¯¢å¹¶ä¸ç›¸å…³ï¼›\n",
    "- é€šè¿‡è¿‡æ»¤æ— å…³å·¥å…·æ¥å‡å°‘ token ä½¿ç”¨é‡ï¼›\n",
    "- æé«˜æ¨¡å‹çš„ä¸“æ³¨åº¦å’Œå‡†ç¡®æ€§ã€‚\n",
    "\n",
    "è¯¥ä¸­é—´ä»¶åˆ©ç”¨ç»“æ„åŒ–è¾“å‡ºï¼Œè®© LLM åˆ¤æ–­å½“å‰æŸ¥è¯¢æœ€ç›¸å…³çš„å·¥å…·æ˜¯å“ªäº›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7a864",
   "metadata": {},
   "source": [
    "## æ„é€ å‚æ•°\n",
    "\n",
    "| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |\n",
    "|------|------|--------|------|\n",
    "| `model` | str \\| BaseChatModel \\| None | None | ç”¨äºé€‰æ‹©çš„æ¨¡å‹ã€‚å¦‚æœæœªæä¾›ï¼Œä½¿ç”¨ä»£ç†çš„ä¸»æ¨¡å‹ã€‚å¯ä»¥æ˜¯æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²æˆ– BaseChatModel å®ä¾‹ |\n",
    "| `system_prompt` | str | `DEFAULT_SYSTEM_PROMPT` | é€‰æ‹©æ¨¡å‹çš„æŒ‡ä»¤ |\n",
    "| `max_tools` | int \\| None | None | è¦é€‰æ‹©çš„æœ€å¤§å·¥å…·æ•°ã€‚å¦‚æœæ¨¡å‹é€‰æ‹©äº†æ›´å¤šï¼Œä»…ä½¿ç”¨å‰ `max_tools` ä¸ªã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™æ²¡æœ‰é™åˆ¶ |\n",
    "| `always_include` | list[str] \\| None | None | æ— è®ºé€‰æ‹©å¦‚ä½•éƒ½å§‹ç»ˆåŒ…å«çš„å·¥å…·åç§°ã€‚è¿™äº›ä¸è®¡å…¥ `max_tools` é™åˆ¶ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dec109",
   "metadata": {},
   "source": [
    "## æ ¸å¿ƒæ–¹æ³•\n",
    "\n",
    "| æ–¹æ³• | è¯´æ˜ |\n",
    "|------|------|\n",
    "| `_prepare_selection_request` | å‡†å¤‡å·¥å…·é€‰æ‹©çš„è¾“å…¥ |\n",
    "| `_process_selection_response` | å¤„ç†é€‰æ‹©å“åº”å¹¶è¿”å›è¿‡æ»¤åçš„ ModelRequest |\n",
    "| `wrap_model_call` /`awrap_model_call` | åœ¨é€šè¿‡å¤„ç†å™¨è°ƒç”¨æ¨¡å‹ä¹‹å‰ï¼ŒåŸºäº LLM é€‰æ‹©è¿‡æ»¤å·¥å…· |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359770d9",
   "metadata": {},
   "source": [
    "## å·¥ä½œæµç¨‹\n",
    "\n",
    "1. `_prepare_selection_request` é€»è¾‘\n",
    "\n",
    "```python\n",
    "def _prepare_selection_request(self, request: ModelRequest) -> _SelectionRequest | None:\n",
    "    # 1. æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨å·¥å…·\n",
    "    if not request.tools or len(request.tools) == 0:\n",
    "        return None\n",
    "\n",
    "    # 2. è¿‡æ»¤å‡º BaseTool å®ä¾‹ï¼ˆæ’é™¤æä¾›å•†ç‰¹å®šçš„å·¥å…·å­—å…¸ï¼‰\n",
    "    base_tools = [tool for tool in request.tools if not isinstance(tool, dict)]\n",
    "\n",
    "    # 3. éªŒè¯ always_include å·¥å…·æ˜¯å¦å­˜åœ¨\n",
    "    if self.always_include:\n",
    "        available_tool_names = {tool.name for tool in base_tools}\n",
    "        missing_tools = [\n",
    "            name for name in self.always_include if name not in available_tool_names\n",
    "        ]\n",
    "        if missing_tools:\n",
    "            raise ValueError(f\"Tools in always_include not found: {missing_tools}\")\n",
    "\n",
    "    # 4. åˆ†ç¦»å§‹ç»ˆåŒ…å«çš„å·¥å…·å’Œå¯ä¾›é€‰æ‹©çš„å·¥å…·\n",
    "    available_tools = [tool for tool in base_tools if tool.name not in self.always_include]\n",
    "\n",
    "    # 5. å¦‚æœæ²¡æœ‰å¯ä¾›é€‰æ‹©çš„å·¥å…·ï¼Œè¿”å› None\n",
    "    if not available_tools:\n",
    "        return None\n",
    "\n",
    "    # 6. å‡†å¤‡ç³»ç»Ÿæ¶ˆæ¯\n",
    "    system_message = self.system_prompt\n",
    "    if self.max_tools is not None:\n",
    "        system_message += (\n",
    "            f\"\\nIMPORTANT: List the tool names in order of relevance, \"\n",
    "            f\"with the most relevant first. \"\n",
    "            f\"If you exceed the maximum number of tools, \"\n",
    "            f\"only the first {self.max_tools} will be used.\"\n",
    "        )\n",
    "\n",
    "    # 7. è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯\n",
    "    for message in reversed(request.messages):\n",
    "        if isinstance(message, HumanMessage):\n",
    "            last_user_message = message\n",
    "            break\n",
    "\n",
    "    # 8. è¿”å›é€‰æ‹©è¯·æ±‚\n",
    "    return _SelectionRequest(\n",
    "        available_tools=available_tools,\n",
    "        system_message=system_message,\n",
    "        last_user_message=last_user_message,\n",
    "        model=self.model or request.model,\n",
    "        valid_tool_names=[tool.name for tool in available_tools],\n",
    "    )\n",
    "```\n",
    "\n",
    "2. `_process_selection_response` é€»è¾‘\n",
    "\n",
    "```python\n",
    "def _process_selection_response(\n",
    "    self,\n",
    "    response: dict,\n",
    "    available_tools: list[BaseTool],\n",
    "    valid_tool_names: list[str],\n",
    "    request: ModelRequest,\n",
    ") -> ModelRequest:\n",
    "    # 1. æå–é€‰æ‹©çš„å·¥å…·åç§°\n",
    "    selected_tool_names: list[str] = []\n",
    "    invalid_tool_selections = []\n",
    "\n",
    "    # 2. éªŒè¯é€‰æ‹©çš„å·¥å…·\n",
    "    for tool_name in response[\"tools\"]:\n",
    "        if tool_name not in valid_tool_names:\n",
    "            invalid_tool_selections.append(tool_name)\n",
    "            continue\n",
    "\n",
    "        # 3. åº”ç”¨ max_tools é™åˆ¶\n",
    "        if tool_name not in selected_tool_names and (\n",
    "            self.max_tools is None or len(selected_tool_names) < self.max_tools\n",
    "        ):\n",
    "            selected_tool_names.append(tool_name)\n",
    "\n",
    "    # 4. æ£€æŸ¥æ— æ•ˆé€‰æ‹©\n",
    "    if invalid_tool_selections:\n",
    "        raise ValueError(f\"Model selected invalid tools: {invalid_tool_selections}\")\n",
    "\n",
    "    # 5. è¿‡æ»¤å·¥å…·\n",
    "    selected_tools: list[BaseTool] = [\n",
    "        tool for tool in available_tools if tool.name in selected_tool_names\n",
    "    ]\n",
    "\n",
    "    # 6. æ·»åŠ  always_include å·¥å…·\n",
    "    always_included_tools: list[BaseTool] = [\n",
    "        tool\n",
    "        for tool in request.tools\n",
    "        if not isinstance(tool, dict) and tool.name in self.always_include\n",
    "    ]\n",
    "    selected_tools.extend(always_included_tools)\n",
    "\n",
    "    # 7. ä¿ç•™æä¾›å•†ç‰¹å®šçš„å·¥å…·å­—å…¸\n",
    "    provider_tools = [tool for tool in request.tools if isinstance(tool, dict)]\n",
    "\n",
    "    # 8. è¿”å›ä¿®æ”¹åçš„è¯·æ±‚\n",
    "    return request.override(tools=[*selected_tools, *provider_tools])\n",
    "```\n",
    "\n",
    "3. `wrap_model_call` é€»è¾‘\n",
    "\n",
    "```python\n",
    "def wrap_model_call(\n",
    "    self,\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelCallResult:\n",
    "    # 1. å‡†å¤‡é€‰æ‹©è¯·æ±‚\n",
    "    selection_request = self._prepare_selection_request(request)\n",
    "    if selection_request is None:\n",
    "        return handler(request)\n",
    "\n",
    "    # 2. åˆ›å»ºåŠ¨æ€å“åº”æ¨¡å‹\n",
    "    type_adapter = _create_tool_selection_response(selection_request.available_tools)\n",
    "    schema = type_adapter.json_schema()\n",
    "    structured_model = selection_request.model.with_structured_output(schema)\n",
    "\n",
    "    # 3. è°ƒç”¨é€‰æ‹©æ¨¡å‹\n",
    "    response = structured_model.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": selection_request.system_message},\n",
    "            selection_request.last_user_message,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 4. éªŒè¯å“åº”\n",
    "    if not isinstance(response, dict):\n",
    "        raise AssertionError(f\"Expected dict response, got {type(response)}\")\n",
    "\n",
    "    # 5. å¤„ç†é€‰æ‹©å“åº”\n",
    "    modified_request = self._process_selection_response(\n",
    "        response,\n",
    "        selection_request.available_tools,\n",
    "        selection_request.valid_tool_names,\n",
    "        request,\n",
    "    )\n",
    "\n",
    "    # 6. è°ƒç”¨å¤„ç†å™¨\n",
    "    return handler(modified_request)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40e82a",
   "metadata": {},
   "source": [
    "## ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "232a2e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "æµ‹è¯• 1: æŸ¥è¯¢å¤©æ°”\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ [Debug 1] ğŸš€ ====== LLMToolSelectorMiddleware å¼€å§‹ ======\n",
      "ğŸ”§ [Debug 1] 1. ä¼ å…¥çš„å·¥å…·: ['check_weather', 'query_database', 'search_knowledge_base']\n",
      "ğŸ”§ [Debug 1] 2. é€‰æ‹©æ¨¡å‹çœ‹åˆ°çš„å·¥å…·: ['check_weather', 'query_database', 'search_knowledge_base']\n",
      "ğŸ”§ [Debug 1] 3. è°ƒç”¨ super().wrap_model_call()...\n",
      "ğŸ“ [Log 1] ğŸš€ ====== LogSelectedTools å¼€å§‹ ======\n",
      "ğŸ“ [Log 1] çœ‹åˆ°çš„å·¥å…·: ['check_weather']\n",
      "ğŸ“ [Log 1] è°ƒç”¨ handler...\n",
      "ğŸ“ [Log 1] handler è¿”å›\n",
      "ğŸ“ [Log 1] ğŸš— ====== LogSelectedTools å®Œæˆ ======\n",
      "\n",
      "ğŸ”§ [Debug 1] 4. super().wrap_model_call() è¿”å›\n",
      "ğŸ”§ [Debug 1] 5. result ç±»å‹: <class 'langchain.agents.middleware.types.ModelResponse'>\n",
      "ğŸ”§ [Debug 1] 6. result å†…å®¹: ModelResponse(result=[AIMessage(content='æˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æƒ…å†µã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 319, 'total_tokens': 371, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 256}, 'prompt_cache_hit_tokens': 256, 'prompt_cache_miss_tokens': 63}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': '71f9ade6-a213-4a75-818a-98f460bd7948', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c3daa-9aac-7d93-971f-e8e4aad72dcf-0', tool_calls=[{'name': 'check_weather', 'args': {'city': 'åŒ—äº¬'}, 'id': 'call_00_62Xd0OcnUOSznbyDZCjO6Cgl', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 319, 'output_tokens': 52, 'total_tokens': 371, 'input_token_details': {'cache_read': 256}, 'output_token_details': {}})], structured_response=None)\n",
      "ğŸ”§ [Debug 1] ğŸš— ====== LLMToolSelectorMiddleware å®Œæˆ ======\n",
      "\n",
      "\n",
      "ğŸ”§ [Debug 2] ğŸš€ ====== LLMToolSelectorMiddleware å¼€å§‹ ======\n",
      "ğŸ”§ [Debug 2] 1. ä¼ å…¥çš„å·¥å…·: ['check_weather', 'query_database', 'search_knowledge_base']\n",
      "ğŸ”§ [Debug 2] 2. é€‰æ‹©æ¨¡å‹çœ‹åˆ°çš„å·¥å…·: ['check_weather', 'query_database', 'search_knowledge_base']\n",
      "ğŸ”§ [Debug 2] 3. è°ƒç”¨ super().wrap_model_call()...\n",
      "ğŸ“ [Log 2] ğŸš€ ====== LogSelectedTools å¼€å§‹ ======\n",
      "ğŸ“ [Log 2] çœ‹åˆ°çš„å·¥å…·: ['check_weather']\n",
      "ğŸ“ [Log 2] è°ƒç”¨ handler...\n",
      "ğŸ“ [Log 2] handler è¿”å›\n",
      "ğŸ“ [Log 2] ğŸš— ====== LogSelectedTools å®Œæˆ ======\n",
      "\n",
      "ğŸ”§ [Debug 2] 4. super().wrap_model_call() è¿”å›\n",
      "ğŸ”§ [Debug 2] 5. result ç±»å‹: <class 'langchain.agents.middleware.types.ModelResponse'>\n",
      "ğŸ”§ [Debug 2] 6. result å†…å®¹: ModelResponse(result=[AIMessage(content='æ ¹æ®æŸ¥è¯¢ç»“æœï¼ŒåŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œæ¸©åº¦25Â°Cã€‚å¤©æ°”ä¸é”™ï¼Œæ¸©åº¦é€‚å®œï¼Œæ˜¯ä¸ªå¥½å¤©æ°”ï¼', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 395, 'total_tokens': 420, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 384}, 'prompt_cache_hit_tokens': 384, 'prompt_cache_miss_tokens': 11}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': '2f1388f3-2690-4950-8e28-99d72a7a7271', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c3daa-ab98-74c1-8a99-99a4b0e8db5e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 395, 'output_tokens': 25, 'total_tokens': 420, 'input_token_details': {'cache_read': 384}, 'output_token_details': {}})], structured_response=None)\n",
      "ğŸ”§ [Debug 2] ğŸš— ====== LLMToolSelectorMiddleware å®Œæˆ ======\n",
      "\n",
      "\n",
      "ğŸ’¬ å›å¤: æ ¹æ®æŸ¥è¯¢ç»“æœï¼ŒåŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œæ¸©åº¦25Â°Cã€‚å¤©æ°”ä¸é”™ï¼Œæ¸©åº¦é€‚å®œï¼Œæ˜¯ä¸ªå¥½å¤©æ°”ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import LLMToolSelectorMiddleware, AgentMiddleware\n",
    "from langchain.agents.middleware.types import ModelRequest, ModelResponse, ModelCallResult\n",
    "from langchain_core.tools import tool\n",
    "from typing import Callable\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.7)\n",
    "\n",
    "@tool\n",
    "def check_weather(city: str) -> str:\n",
    "    \"\"\"æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ\n",
    "    \n",
    "    Args:\n",
    "        city: åŸå¸‚åç§°\n",
    "        \n",
    "    Returns:\n",
    "        å¤©æ°”ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    weather_data = {\n",
    "        \"åŒ—äº¬\": \"æ™´å¤©ï¼Œæ¸©åº¦ 25Â°C\",\n",
    "        \"ä¸Šæµ·\": \"å¤šäº‘ï¼Œæ¸©åº¦ 22Â°C\",\n",
    "        \"å¹¿å·\": \"é˜´å¤©ï¼Œæ¸©åº¦ 28Â°C\",\n",
    "        \"æ·±åœ³\": \"æ™´å¤©ï¼Œæ¸©åº¦ 27Â°C\",\n",
    "    }\n",
    "    return weather_data.get(city, f\"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ° {city} çš„å¤©æ°”ä¿¡æ¯\")\n",
    "\n",
    "@tool\n",
    "def query_database(table: str, condition: str = \"\") -> str:\n",
    "    \"\"\"æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®\n",
    "    \n",
    "    Args:\n",
    "        table: è¡¨å\n",
    "        condition: æŸ¥è¯¢æ¡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "        æŸ¥è¯¢ç»“æœ\n",
    "    \"\"\"\n",
    "    database_data = {\n",
    "        \"users\": \"ç”¨æˆ·è¡¨åŒ…å« 1000 æ¡è®°å½•\",\n",
    "        \"orders\": \"è®¢å•è¡¨åŒ…å« 5000 æ¡è®°å½•\",\n",
    "        \"products\": \"äº§å“è¡¨åŒ…å« 200 æ¡è®°å½•\",\n",
    "    }\n",
    "    if condition:\n",
    "        return f\"ä» {table} è¡¨æŸ¥è¯¢ï¼Œæ¡ä»¶: {condition}ï¼Œç»“æœ: {database_data.get(table, 'è¡¨ä¸å­˜åœ¨')}\"\n",
    "    return f\"æŸ¥è¯¢ {table} è¡¨ï¼Œç»“æœ: {database_data.get(table, 'è¡¨ä¸å­˜åœ¨')}\"\n",
    "\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯\n",
    "    \n",
    "    Args:\n",
    "        query: æœç´¢å…³é”®è¯æˆ–é—®é¢˜\n",
    "        \n",
    "    Returns:\n",
    "        çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    knowledge_base = {\n",
    "        \"å…¬å¸\": \"æˆ‘ä»¬æ˜¯ä¸€å®¶ä¸“æ³¨äº AI æŠ€æœ¯çš„å…¬å¸\",\n",
    "        \"äº§å“\": \"ä¸»è¦äº§å“åŒ…æ‹¬æ™ºèƒ½å®¢æœã€æ•°æ®åˆ†æå¹³å°ç­‰\",\n",
    "        \"æŠ€æœ¯\": \"ä½¿ç”¨æœ€æ–°çš„æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯\",\n",
    "        \"æœåŠ¡\": \"æä¾› 7x24 å°æ—¶æŠ€æœ¯æ”¯æŒæœåŠ¡\",\n",
    "    }\n",
    "    for key, value in knowledge_base.items():\n",
    "        if key in query:\n",
    "            return f\"çŸ¥è¯†åº“æœç´¢ç»“æœ: {value}\"\n",
    "    return f\"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°å…³äº '{query}' çš„ä¿¡æ¯\"\n",
    "\n",
    "\n",
    "class DebugToolSelectorMiddleware(LLMToolSelectorMiddleware):\n",
    "    \"\"\"è°ƒè¯•å·¥å…·é€‰æ‹©ä¸­é—´ä»¶\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._call_count = 0\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelCallResult:\n",
    "        self._call_count += 1\n",
    "        call_id = self._call_count\n",
    "        print(f\"\\nğŸ”§ [Debug {call_id}] ğŸš€ ====== LLMToolSelectorMiddleware å¼€å§‹ ======\")\n",
    "        print(f\"ğŸ”§ [Debug {call_id}] 1. ä¼ å…¥çš„å·¥å…·: {[tool.name for tool in request.tools]}\")\n",
    "        \n",
    "        selection_request = self._prepare_selection_request(request)\n",
    "        if selection_request is not None:\n",
    "            print(f\"ğŸ”§ [Debug {call_id}] 2. é€‰æ‹©æ¨¡å‹çœ‹åˆ°çš„å·¥å…·: {[tool.name for tool in selection_request.available_tools]}\")\n",
    "        \n",
    "        print(f\"ğŸ”§ [Debug {call_id}] 3. è°ƒç”¨ super().wrap_model_call()...\")\n",
    "        result = super().wrap_model_call(request, handler)\n",
    "        print(f\"ğŸ”§ [Debug {call_id}] 4. super().wrap_model_call() è¿”å›\")\n",
    "        \n",
    "        print(f\"ğŸ”§ [Debug {call_id}] 5. result ç±»å‹: {type(result)}\")\n",
    "        print(f\"ğŸ”§ [Debug {call_id}] 6. result å†…å®¹: {result}\")\n",
    "        if hasattr(result, 'response'):\n",
    "            print(f\"ğŸ”§ [Debug {call_id}] 7. result.response ç±»å‹: {type(result.response)}\")\n",
    "            if hasattr(result.response, 'content'):\n",
    "                print(f\"ğŸ”§ [Debug {call_id}] 8. é€‰æ‹©æ¨¡å‹çš„é€‰æ‹©: {result.response.content}\")\n",
    "        \n",
    "        print(f\"ğŸ”§ [Debug {call_id}] ğŸš— ====== LLMToolSelectorMiddleware å®Œæˆ ======\\n\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class LogSelectedTools(AgentMiddleware):\n",
    "    \"\"\"è®°å½•é€‰æ‹©çš„å·¥å…·\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._call_count = 0\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelCallResult:\n",
    "        self._call_count += 1\n",
    "        call_id = self._call_count\n",
    "        print(f\"ğŸ“ [Log {call_id}] ğŸš€ ====== LogSelectedTools å¼€å§‹ ======\")\n",
    "        print(f\"ğŸ“ [Log {call_id}] çœ‹åˆ°çš„å·¥å…·: {[tool.name for tool in request.tools]}\")\n",
    "        print(f\"ğŸ“ [Log {call_id}] è°ƒç”¨ handler...\")\n",
    "        \n",
    "        result = handler(request)\n",
    "        \n",
    "        print(f\"ğŸ“ [Log {call_id}] handler è¿”å›\")\n",
    "        print(f\"ğŸ“ [Log {call_id}] ğŸš— ====== LogSelectedTools å®Œæˆ ======\\n\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "tool_selector = DebugToolSelectorMiddleware(\n",
    "    model=model,\n",
    "    max_tools=1\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[check_weather, query_database, search_knowledge_base],\n",
    "    middleware=[tool_selector, LogSelectedTools()]\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"æµ‹è¯• 1: æŸ¥è¯¢å¤©æ°”\")\n",
    "print(\"=\" * 50)\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\")]\n",
    "})\n",
    "print(f\"\\nğŸ’¬ å›å¤: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1d44f",
   "metadata": {},
   "source": [
    "æµç¨‹æ€»ç»“ï¼šé…ç½®é¡ºåº`[tool_selector, LogSelectedTools]`æ„å‘³ç€ tool_selector åœ¨å¤–å±‚ï¼Œ LogSelectedTools åœ¨å†…å±‚ã€‚ tool_selectorå…ˆæ‰§è¡Œwrap_model_callï¼Œè¿‡æ»¤å·¥å…·åä¼ ç»™ LogSelectedTools.wrap_model_callï¼Œæ‰€ä»¥ LogSelectedToolsçœ‹åˆ°çš„æ˜¯è¿‡æ»¤åçš„1ä¸ªå·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56366f",
   "metadata": {},
   "source": [
    "# RetryMiddleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10437f5",
   "metadata": {},
   "source": [
    "ToolRetryMiddleware/ModelRetryMiddlewareåˆå¹¶åˆ°ä¸€èµ·æ¥ä»‹ç»ï¼Œä»–ä»¬éƒ½æ”¯æŒè‡ªåŠ¨å¯¹å¤±è´¥çš„è°ƒç”¨è¿›è¡Œé‡è¯•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68abfeb",
   "metadata": {},
   "source": [
    "| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ModelRetryMiddleware | ToolRetryMiddleware | è¯´æ˜ |\n",
    "|------|------|--------|---------------------|---------------------|------|\n",
    "| `max_retries` | `number` | `2` | âœ… | âœ… | åˆå§‹è°ƒç”¨å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤å…±å°è¯•3æ¬¡ï¼ŒåŒ…æ‹¬åˆå§‹è°ƒç”¨ï¼‰ |\n",
    "| `tools` | `list[BaseTool \\| str]` | `None` | âŒ | âœ… | å¯é€‰çš„å·¥å…·åˆ—è¡¨æˆ–å·¥å…·åç§°åˆ—è¡¨ï¼Œç”¨äºæŒ‡å®šåº”ç”¨é‡è¯•é€»è¾‘çš„èŒƒå›´ã€‚è‹¥ä¸º`None`ï¼Œåˆ™å¯¹æ‰€æœ‰å·¥å…·ç”Ÿæ•ˆ |\n",
    "| `retry_on` | `tuple[type[Exception], ...] \\| callable` | `(Exception,)` | âœ… | âœ… | å¯ä¸ºä¸€ä¸ªå¼‚å¸¸ç±»å‹å…ƒç»„ï¼Œè¡¨ç¤ºåœ¨è¿™äº›å¼‚å¸¸å‘ç”Ÿæ—¶è¿›è¡Œé‡è¯•ï¼›ä¹Ÿå¯ä¸ºä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œæ¥æ”¶å¼‚å¸¸å®ä¾‹å¹¶è¿”å›`True`è¡¨ç¤ºåº”é‡è¯• |\n",
    "| `on_failure` | `string \\| callable` | `\"return_message\"` | âœ… | âœ… | å½“æ‰€æœ‰é‡è¯•å‡å¤±è´¥åçš„å¤„ç†è¡Œä¸ºã€‚é€‰é¡¹åŒ…æ‹¬ï¼š`'return_message'`ã€`'raise'`ã€è‡ªå®šä¹‰å¯è°ƒç”¨å¯¹è±¡ |\n",
    "| `backoff_factor` | `number` | `2.0` | âœ… | âœ… | æŒ‡æ•°é€€é¿çš„ä¹˜æ•°å› å­ã€‚æ¯æ¬¡é‡è¯•çš„ç­‰å¾…æ—¶é—´ä¸º `initial_delay * (backoff_factor ** retry_number)` ç§’ã€‚è®¾ä¸º`0.0`è¡¨ç¤ºä½¿ç”¨å›ºå®šå»¶è¿Ÿã€‚éšç€å¤±è´¥æ¬¡æ•°å¢åŠ ï¼Œé€æ­¥æ‹‰é•¿ç­‰å¾…æ—¶é—´ï¼Œç»™å¤–éƒ¨ç³»ç»Ÿæ›´å¤šæ¢å¤æœºä¼š |\n",
    "| `initial_delay` | `number` | `1.0` | âœ… | âœ… | é¦–æ¬¡é‡è¯•å‰çš„åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰ |\n",
    "| `max_delay` | `number` | `60.0` | âœ… | âœ… | é‡è¯•ä¹‹é—´å»¶è¿Ÿçš„æœ€å¤§å€¼ï¼ˆå•ä½ï¼šç§’ï¼‰ï¼Œç”¨äºé™åˆ¶æŒ‡æ•°é€€é¿çš„å¢é•¿ä¸Šé™ |\n",
    "| `jitter` | `boolean` | `true` | âœ… | âœ… | æ˜¯å¦åœ¨å»¶è¿Ÿä¸­åŠ å…¥éšæœºæŠ–åŠ¨ï¼ˆÂ±25%ï¼‰ï¼Œä»¥é¿å…\"æƒŠç¾¤æ•ˆåº”\"ï¼ˆthundering herdï¼‰ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde9e77",
   "metadata": {},
   "source": [
    "**ToolRetryMiddleware æ¯” ModelRetryMiddleware å¤šä¸€ä¸ª `tools` å‚æ•°ï¼Œç”¨äºæŒ‡å®šåªé‡è¯•ç‰¹å®šå·¥å…·ã€‚**\n",
    "\n",
    "- `tools=None`ï¼šé‡è¯•æ‰€æœ‰å·¥å…·ï¼ˆé»˜è®¤ï¼‰\n",
    "- `tools=[\"tool1\", \"tool2\"]`ï¼šåªé‡è¯•æŒ‡å®šåç§°çš„å·¥å…·\n",
    "- `tools=[search_tool, db_tool]`ï¼šåªé‡è¯•æŒ‡å®šçš„å·¥å…·å®ä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3788d",
   "metadata": {},
   "source": [
    "å·¥å…·é‡è¯•ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b442f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ToolRetryMiddleware\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_tool, database_tool, api_tool],\n",
    "    middleware=[\n",
    "        ToolRetryMiddleware(\n",
    "            max_retries=3,\n",
    "            backoff_factor=2.0,\n",
    "            initial_delay=1.0,\n",
    "            max_delay=60.0,\n",
    "            jitter=True,\n",
    "            tools=[\"api_tool\"],\n",
    "            retry_on=(ConnectionError, TimeoutError),\n",
    "            on_failure=\"continue\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62b5c8",
   "metadata": {},
   "source": [
    "æ¨¡å‹é‡è¯•ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a84f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelRetryMiddleware\n",
    "\n",
    "\n",
    "# åŸºç¡€ç”¨æ³•ï¼šä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ˆ2æ¬¡é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿ï¼‰\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_tool],\n",
    "    middleware=[ModelRetryMiddleware()],\n",
    ")\n",
    "\n",
    "# è‡ªå®šä¹‰å¼‚å¸¸è¿‡æ»¤\n",
    "class TimeoutError(Exception):\n",
    "    \"\"\"è‡ªå®šä¹‰è¶…æ—¶å¼‚å¸¸ã€‚\"\"\"\n",
    "    pass\n",
    "\n",
    "class ConnectionError(Exception):\n",
    "    \"\"\"è‡ªå®šä¹‰è¿æ¥å¼‚å¸¸ã€‚\"\"\"\n",
    "    pass\n",
    "\n",
    "# ä»…é’ˆå¯¹ç‰¹å®šå¼‚å¸¸è¿›è¡Œé‡è¯•\n",
    "retry = ModelRetryMiddleware(\n",
    "    max_retries=4, # æœ€å¤šé‡è¯•4æ¬¡\n",
    "    retry_on=(TimeoutError, ConnectionError), # ä»…åœ¨å‘ç”Ÿè¿™ä¸¤ç§å¼‚å¸¸æ—¶é‡è¯•\n",
    "    backoff_factor=1.5, # é€€é¿å› å­\n",
    ")\n",
    "\n",
    "\n",
    "# è‡ªå®šä¹‰é‡è¯•åˆ¤æ–­é€»è¾‘\n",
    "def should_retry(error: Exception) -> bool:\n",
    "    # ä»…åœ¨å‘ç”Ÿé™æµé”™è¯¯æ—¶é‡è¯•\n",
    "    if isinstance(error, TimeoutError):\n",
    "        return True\n",
    "    # æˆ–è€…æ£€æŸ¥ç‰¹å®šçš„ HTTP çŠ¶æ€ç \n",
    "    if hasattr(error, \"status_code\"):\n",
    "        return error.status_code in (429, 503) # 429: é™æµ, 503: æœåŠ¡ä¸å¯ç”¨\n",
    "    return False\n",
    "\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰è¿‡æ»¤å‡½æ•°\n",
    "retry_with_filter = ModelRetryMiddleware(\n",
    "    max_retries=3,\n",
    "    retry_on=should_retry,\n",
    ")\n",
    "\n",
    "# å¤±è´¥åè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸\n",
    "retry_continue = ModelRetryMiddleware(\n",
    "    max_retries=4,\n",
    "    on_failure=\"continue\",  # å‘ç”Ÿå¤±è´¥æ—¶è¿”å› AIMessage é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸\n",
    ")\n",
    "\n",
    "# è‡ªå®šä¹‰é”™è¯¯ä¿¡æ¯æ ¼å¼\n",
    "def format_error(error: Exception) -> str:\n",
    "    return f\"æ¨¡å‹è°ƒç”¨å¤±è´¥: {error}ã€‚è¯·ç¨åå†è¯•ã€‚\"\n",
    "\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–å‡½æ•°\n",
    "retry_with_formatter = ModelRetryMiddleware(\n",
    "    max_retries=4,\n",
    "    on_failure=format_error,\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨å›ºå®šé€€é¿æ—¶é—´ï¼ˆæ— æŒ‡æ•°å¢é•¿ï¼‰\n",
    "constant_backoff = ModelRetryMiddleware(\n",
    "    max_retries=5,\n",
    "    backoff_factor=0.0,  # ç¦ç”¨æŒ‡æ•°å¢é•¿\n",
    "    initial_delay=2.0,  # æ¯æ¬¡å›ºå®šç­‰å¾… 2 ç§’\n",
    ")\n",
    "\n",
    "# ä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥æ—¶ç›´æ¥æŠ›å‡ºå¼‚å¸¸\n",
    "strict_retry = ModelRetryMiddleware(\n",
    "    max_retries=2,\n",
    "    on_failure=\"error\",  # å‘ç”Ÿå¤±è´¥æ—¶ç›´æ¥é‡æ–°æŠ›å‡ºå¼‚å¸¸\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0072c95",
   "metadata": {},
   "source": [
    "# LLMToolEmulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0f5da",
   "metadata": {},
   "source": [
    "ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œï¼Œç”¨äºæµ‹è¯•ç›®çš„ï¼Œå°†å®é™…çš„å·¥å…·è°ƒç”¨æ›¿æ¢ä¸º AI ç”Ÿæˆçš„å“åº”ã€‚LLM å·¥å…·æ¨¡æ‹Ÿå™¨é€‚ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š\n",
    "- åœ¨ä¸æ‰§è¡ŒçœŸå®å·¥å…·çš„æƒ…å†µä¸‹æµ‹è¯•æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼›\n",
    "- åœ¨å¤–éƒ¨å·¥å…·ä¸å¯ç”¨æˆ–è°ƒç”¨æˆæœ¬è¾ƒé«˜æ—¶å¼€å‘æ™ºèƒ½ä½“ï¼›\n",
    "- åœ¨å®ç°çœŸå®å·¥å…·ä¹‹å‰ï¼Œå¯¹æ™ºèƒ½ä½“å·¥ä½œæµè¿›è¡ŒåŸå‹è®¾è®¡å’ŒéªŒè¯ã€‚\n",
    "\n",
    "åº•å±‚æ˜¯ä¾é wrap_tool_callæ¥å®ç°çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bc92e",
   "metadata": {},
   "source": [
    "| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |\n",
    "|------|------|--------|------|\n",
    "| `tools` | `list[str \\| BaseTool] \\| None` | `None` | è¦æ¨¡æ‹Ÿçš„å·¥å…·åˆ—è¡¨ã€‚`None` è¡¨ç¤ºæ¨¡æ‹Ÿæ‰€æœ‰å·¥å…·ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºä¸æ¨¡æ‹Ÿä»»ä½•å·¥å…· |\n",
    "| `model` | `str \\| BaseChatModel \\| None` | `'anthropic:claude-sonnet-4-5-20250929'` | ç”¨äºæ¨¡æ‹Ÿçš„æ¨¡å‹ã€‚å¯ä»¥æ˜¯æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²æˆ– `BaseChatModel` å®ä¾‹ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7abfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import LLMToolEmulator\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "# å®šä¹‰è·å–å¤©æ°”çš„å·¥å…·\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"è·å–æŒ‡å®šåœ°ç‚¹çš„å½“å‰å¤©æ°”ã€‚\"\"\"\n",
    "    return f\"Weather in {location}\"\n",
    "\n",
    "# å®šä¹‰å‘é€é‚®ä»¶çš„å·¥å…·\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"å‘é€é‚®ä»¶ã€‚\"\"\"\n",
    "    return \"Email sent\"\n",
    "\n",
    "\n",
    "# æ–¹å¼ä¸€ï¼šæ¨¡æ‹Ÿæ‰€æœ‰å·¥å…·ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰\n",
    "# å¦‚æœ LLM æ— æ³•è°ƒç”¨å·¥å…·ï¼Œä¸­é—´ä»¶ä¼šå°è¯•ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ‹Ÿå·¥å…·çš„æ‰§è¡Œç»“æœ\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, send_email],\n",
    "    middleware=[LLMToolEmulator()],\n",
    ")\n",
    "\n",
    "# æ–¹å¼äºŒï¼šä»…æ¨¡æ‹ŸæŒ‡å®šçš„å·¥å…·\n",
    "# åªå¯¹ get_weather å·¥å…·è¿›è¡Œæ¨¡æ‹Ÿï¼Œå…¶ä»–å·¥å…·ä¿æŒåŸæ ·\n",
    "agent2 = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, send_email],\n",
    "    middleware=[LLMToolEmulator(tools=[\"get_weather\"])],\n",
    ")\n",
    "\n",
    "# æ–¹å¼å››ï¼šä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è¿›è¡Œæ¨¡æ‹Ÿ\n",
    "# æŒ‡å®šä½¿ç”¨ claude æ¨¡å‹æ¥æ¨¡æ‹Ÿå·¥å…·çš„è¡Œä¸º\n",
    "agent4 = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, send_email],\n",
    "    middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa0ef8",
   "metadata": {},
   "source": [
    "# ContextEditingMiddleware/ClearToolUsesEdit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715237a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
